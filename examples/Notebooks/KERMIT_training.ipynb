{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using kerMIT - Try Model -\n",
    "\n",
    "This notebook explains how train and test KERMIT_model and save the weights to subsequently use KERMITviz.\n",
    "\n",
    "* *Note: In the previous notebook we used ag_news as an example dataset, but please note that the user can choose one as he prefers.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages\n",
    "Before starting, it is essential to have the following requirements:\n",
    "- transformers==2.6.0\n",
    "- torch and torchtext\n",
    "- pycuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==2.6.0\n",
    "#!pip install torch\n",
    "#!pip install torchtext\n",
    "#!pip install pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, pickle, copy, transformers\n",
    "from torchtext import data as datx\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "#set manual seed for replicability\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(23)\n",
    "\n",
    "#insert path of dataset\n",
    "dataPath = ''\n",
    "\n",
    "#training set of encoded trees \n",
    "nameTree1 = 'dtk_trees_ag_news_train.pkl'\n",
    "\n",
    "#test set of encoded trees \n",
    "nameTree2 = 'dtk_trees_ag_news_test.pkl'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "To standardise the input we have chosen the csv format.\n",
    "\n",
    "If the user wants to use his own dataset he will have to modify this part and the section concerning data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dataset_train = 'train.csv'\n",
    "\n",
    "name_dataset_test = 'test.csv'\n",
    "\n",
    "#show the dataset\n",
    "data = pd.read_csv(dataPath+name_dataset)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "PyTorch includes packages to prepare and load common datasets for your model.\n",
    "In the next sections the functions for loading are defined \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeField(datx.Field):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "\n",
    "        def preprocess(self, x):\n",
    "                return x\n",
    "\n",
    "        def process(self, batch, device=None):\n",
    "                return torch.stack(batch)\n",
    "\n",
    "#unpack .pkl encoded trees\n",
    "def unplickle_trees(path_tree_file):\n",
    "    print('--->read DTKs')\n",
    "    dt_trees = []\n",
    "    with open(path_tree_file, 'rb') as fr:\n",
    "        try:\n",
    "            while True:\n",
    "                dt_trees.append(pickle.load(fr))\n",
    "        except EOFError:\n",
    "            pass\n",
    "    return [torch.FloatTensor(i) for i in dt_trees]\n",
    "\n",
    "\n",
    "def add_parsed_tree(test, test_tree_list, field):\n",
    "        test_Examples_tree_list = []\n",
    "        for tr in test_tree_list:\n",
    "                tree = datx.Example.fromlist([tr], [('Tree', field)])\n",
    "                test_Examples_tree_list.append(tree)\n",
    "        test.fields['Tree'] = field\n",
    "        new_test_examples_list = []\n",
    "        for example, tree_ex in zip(test.examples, test_Examples_tree_list):\n",
    "                to_append = example\n",
    "                to_append.Tree = tree_ex.Tree\n",
    "                new_test_examples_list.append(to_append)\n",
    "        test.examples = new_test_examples_list\n",
    "        return test\n",
    "    \n",
    "def first_tree(test, test_tree_list, field):\n",
    "        test_Examples_tree_list = []\n",
    "        tr = test_tree_list[0]\n",
    "        tree = datx.Example.fromlist([tr], [('Tree', field)])\n",
    "        test_Examples_tree_list.append(tree)\n",
    "        test.fields['Tree'] = field\n",
    "        new_test_examples_list = []\n",
    "        for example, tree_ex in zip(test.examples, test_Examples_tree_list):\n",
    "                to_append = example\n",
    "                to_append.Tree = tree_ex.Tree\n",
    "                new_test_examples_list.append(to_append)\n",
    "        test.examples = new_test_examples_list\n",
    "        return test\n",
    "\n",
    "class UnprField(datx.Field):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def preprocess(self, x):\n",
    "        return x\n",
    "\n",
    "    def process(self, batch, device=None):\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer initialization\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "pad_index = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "TEXT = datx.Field(use_vocab=False,fix_length=51, tokenize=tokenizer.encode, pad_token=pad_index, batch_first=True)\n",
    "\n",
    "LABEL = datx.Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "TREE = TreeField(sequential=False, use_vocab=False, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions allow us to load data in previously fixed batches. \n",
    "In both functions (in fields) you should define the columns of the dataset that is used and and state whether the header should be read. whether the header should be read.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 125\n",
    "BATCH_SIZE_test = 32\n",
    "\n",
    "#takes the dataset string as input and returns tuple (train_iter, vocab)\n",
    "def dataset_to_train(dataset, dataPath, nameTree):\n",
    "\n",
    "    LABEL = datx.Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "    fields=[('Label', LABEL),('Title', None), ('Text', TEXT)]\n",
    "    train = datx.TabularDataset(path=f'{dataset}.csv', format='csv',fields=fields, skip_header=True)\n",
    "    train_trees_list = unplickle_trees(f''+dataPath+''+nameTree+'')\n",
    "    train = add_parsed_tree(train, train_trees_list, TREE)\n",
    "    train_iter, a = datx.Iterator.splits(\n",
    "            (train, _), sort_key=lambda x: len(x.Text),\n",
    "            batch_sizes=(BATCH_SIZE, 1))\n",
    "\n",
    "    return (train_iter, a)\n",
    "\n",
    "\n",
    "#takes the dataset string as input and returns tuple (test_iter, vocab)\n",
    "def dataset_to_test(dataset, dataPath, nameTree):\n",
    "\n",
    "    LABEL = datx.Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "    fields=[('Label', LABEL),('Title', None), ('Text', TEXT)]\n",
    "    test = datx.TabularDataset(path=f'{dataset}.csv', format='csv',fields=fields, skip_header=False)\n",
    "    test_trees_list = unplickle_trees(f''+dataPath+''+nameTree+'')\n",
    "    test = add_parsed_tree(test, test_trees_list, TREE)\n",
    "    test_iter, a = datx.Iterator.splits(\n",
    "            (train, _), sort_key=lambda x: len(x.Text),\n",
    "            batch_sizes=(BATCH_SIZE_test, 1))\n",
    "\n",
    "    return (test_iter, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Infer\n",
    "For the train function, we simply have to loop over our data iterator and feed the inputs to the network and optimize.\n",
    "Then we evaluate the performance using infer(), which by taking the test set as input returns the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter, dataset_name):\n",
    "\n",
    "    contEp = 0\n",
    "    lung = len(list(train_iter))\n",
    "    for epoc in (range(EPOCH)):\n",
    "        contEp += 1\n",
    "        running_loss = 0\n",
    "        train_acc = 0\n",
    "        tot = []\n",
    "        for elem in tqdm(iter(train_iter)):\n",
    "            x_sem = elem.Text.cuda()\n",
    "            x_synth = elem.Tree.cuda()\n",
    "            target = elem.Label.cuda()\n",
    "            target_hat = model(x_sem, x_synth)\n",
    "            loss = criterion(target_hat, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            train_acc += (torch.exp(target_hat).argmax(1) == target).sum().item()\n",
    "            res = [1 if x == True else 0 for x in list(torch.exp(target_hat).argmax(1) == target)]\n",
    "            tot += res\n",
    "\n",
    "        print(\"Epoch: \" , contEp)\n",
    "        print(\"Loss: \" + str(running_loss / lung))\n",
    "        print(\"Accuracy: \" + str(sum(tot) / len(tot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(test_iter, neural_model,dataset_name , EPOCH=1, L=30, lambda_norm=0.001):\n",
    "    \n",
    "    running_loss = 0\n",
    "    train_acc = 0\n",
    "    lung = len(list(test_iter))\n",
    "    tot = []\n",
    "    model_infer = DTBert(BERT_DIM, TREE_DIM, OUTPUT_DIM)  \n",
    "    model_infer.load_state_dict(neural_model.state_dict())\n",
    "    model_infer.cuda()\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.AdamW(model_infer.parameters(), lr=1e-3)\n",
    "\n",
    "    for elem in tqdm(iter(test_iter)):\n",
    "        x_sem = elem.Text.cuda()\n",
    "        x_synth = elem.Tree.cuda()\n",
    "        target = elem.Label.cuda()\n",
    "        \n",
    "        with torch.torch.no_grad():\n",
    "            target_hat = model_infer(x_sem, x_synth)\n",
    "            loss = criterion(target_hat, target)\n",
    "            running_loss += loss.item()\n",
    "        train_acc += (torch.exp(target_hat).argmax(1) == target).sum().item()\n",
    "        res = [1 if x == True else 0 for x in list(torch.exp(target_hat).argmax(1) == target)]\n",
    "        tot += res\n",
    "    print(\"Loss: \" + str(running_loss / lung))\n",
    "    print(\"Accuracy: \" + str(sum(tot) / len(tot)))\n",
    "    return(sum(tot) / len(tot))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "The model proposed in this tutorial is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class DTBert(nn.Module):\n",
    "    def __init__(self, input_dim_bert, input_dim_dt, output_dim):\n",
    "        super().__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased').to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.synth_sem_linear = nn.Linear(input_dim_bert + input_dim_dt, output_dim)\n",
    "        \n",
    "    def forward(self, x_sem, x_synth):\n",
    "        with torch.no_grad():\n",
    "            x_sem = self.bert(x_sem)[0][:, 0, :]\n",
    "        x_tot = torch.cat((x_sem, x_synth), 1)\n",
    "        x_tot = self.synth_sem_linear(x_tot)\n",
    "        out = F.log_softmax(x_tot, dim=1)\n",
    "        return out\n",
    "        \n",
    "\n",
    "        \n",
    "BERT_DIM = 768\n",
    "TREE_DIM = 4000\n",
    "\n",
    "OUTPUT_DIM = 5\n",
    "\n",
    "model = DTBert(BERT_DIM, TREE_DIM, OUTPUT_DIM)        \n",
    "model.cuda()\n",
    "\n",
    "#Define a Loss function and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_train = [ dataPath+name_dataset_train]\n",
    "\n",
    "train_list = [] #each element is a tuple (train, test)\n",
    "\n",
    "for dat in datasets_train:\n",
    "    train_list.append(dataset_to_train(dat, dataPath, nameTree1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCH = 5\n",
    "\n",
    "test_accuracies_NO_mem = []\n",
    "\n",
    "for elem, dataset_name in zip(train_list, datasets_train):\n",
    "    print(f\"Training dataset: {dataset_name}\")\n",
    "    train(elem[0], dataset_name, EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_test = [ dataPath+name_dataset_test]\n",
    "\n",
    "test_list = [] #each element is a tuple (train, test)\n",
    "\n",
    "for dat in datasets_test:\n",
    "    test_list.append(dataset_to_test(dat, dataPath, nameTree2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = []\n",
    "\n",
    "for elem, dataset_name in zip(test_list, datasets_test):\n",
    "    print(f\"Testing dataset: {dataset_name}\")\n",
    "    test_accuracy.append(infer(elem[0], model, dataset_name,1))\n",
    "print(\"===================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: ',test_accuracy[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "torch.save(model, 'model.pt')\n",
    "\n",
    "#load weights\n",
    "#model = torch.load('./path/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}